{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cylleneus + NLP\n",
    "===============\n",
    "\n",
    "Once installed, the Cylleneus engine can be used in conjunction with the CLTK to perform queries programmatically via the search API. In this way, the engine can in fact be used to build NLP applications. One very simple and straightforward use of Cylleneus' lemma-based query functionality would be to try to find 'intertexts' -- passages of text that are lexically similar to, but not morphologically identical to, some source text. Let's try it out.\n",
    "\n",
    "First, set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some standard CLTK imports\n",
    "from cltk.tokenize.latin.word import WordTokenizer\n",
    "from cltk.stop.latin import STOPS_LIST\n",
    "STOPS_LIST += ['-que', '-ve', '-ne']\n",
    "\n",
    "# We need tell Cylleneus what corpus we want to search, and then instantiate a Searcher object to execute specific queries.\n",
    "from cylleneus.corpus import Corpus\n",
    "from cylleneus.search import Searcher, Collection\n",
    "\n",
    "# Utility imports\n",
    "from copy import copy\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Perseus Digital Library minicorpus that comes pre-installed with the Cylleneus repository; it includes the major works of Vergil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus('perseus')\n",
    "if not corpus.searchable:\n",
    "    corpus.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to abstract away morphological details of our source text, we are also going to need to tokenize and lemmatize this text. In this case, for simplicity's sake, we will just be searching for a single phrase, which we can input manually. Since the text isn't coming from a structured corpus, we can use the built-in plaintext tokenizer and lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plaintext tokenizer is suitable for 'raw' Latin text.\n",
    "from cylleneus.corpus.default import CachedTokenizer\n",
    "\n",
    "# The lemma filter takes a sequence of tokens (word-forms) and uses the Latin WordNet for lemmatization and morphological analysis.\n",
    "from cylleneus.engine.analysis.filters import CachedLemmaFilter\n",
    "\n",
    "word_tokenizer = WordTokenizer()\n",
    "tokenizer = CachedTokenizer()\n",
    "lemmatizer = CachedLemmaFilter(cached=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our source text through our lemmatization pipeline. In this fabricated example, we are going to search for texts similar to the phrase of Lucretius: *gelidamque pruinam* (Lucr. *RN.* 2.431)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'gelidamque pruinam'\n",
    "\n",
    "# For efficiency the tokenizer reuses a single Token object, so each token needs to be copied to be preserved\n",
    "words = [word for word in word_tokenizer.tokenize(text) if word not in STOPS_LIST]\n",
    "tokens = [copy(token) for token in tokenizer(words, mode='index', tokenize=False)]\n",
    "pprint(tokens)\n",
    "\n",
    "lemmas = []\n",
    "for token in tokens:\n",
    "    lemmatized = set()\n",
    "    for lemma in lemmatizer([copy(token),]):\n",
    "        lemmatized.add(lemma.text.split(':')[0])\n",
    "\n",
    "    lemmas.append(list(lemmatized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we need to construct a well-formed lemma-based query for Cylleneus to execute. We could simply combine the lemmatized tokens together as a sequence.\n",
    "\n",
    "NB. The lemmatizer tries to be inclusive as possible, so a form like *fatis* will generate multiple lemmas for possible matching: *fatum* as well as *fatis* and *fatus*. This is why, if we were to inspect the `lemmas` object, we would find that each word of the original text resolves to a list of lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct sequential lemma-based query\n",
    "subqueries = []\n",
    "for i, lemma in enumerate(lemmas):\n",
    "    # If lemmatization didn't produce anything, use the original form\n",
    "    if len(lemma) == 0:\n",
    "        subqueries.append(tokens[i].text)\n",
    "    elif len(lemma) == 1:\n",
    "        subqueries.append(f\"<{lemma[0]}>\")\n",
    "    else:\n",
    "        subqueries.append(f'''({' OR '.join([f\"<{alt}>\" for alt in lemma])})''')\n",
    "\n",
    "# Join all subqueries into a single adjacency query\n",
    "adjacency_lemmas = f'''\"{' THEN '.join(subqueries)}\"'''\n",
    "pprint(adjacency_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be more inclusive -- and to take account of that intervening *-que* in Lucretius -- we should probably do away with the strict sequential requirement and try instead using a proximity query. In this case, any text will match provided only that it contains the matching query terms, irrespective of their ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proximity_lemmas = f'''{' AND '.join(subqueries)}'''\n",
    "pprint(proximity_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query against the given collection of texts.\n",
    "searcher = Searcher(Collection(works=corpus.works))\n",
    "search = searcher.search(proximity_lemmas)\n",
    "\n",
    "# Display the query if any matches\n",
    "if search.count != (0, 0, 0):  # matches, docs, corpora\n",
    "    for hlite in search.highlights:\n",
    "        pprint([hlite.author,\n",
    "               hlite.title,\n",
    "               hlite.reference,\n",
    "               hlite.text])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's go one step further: finding so-called 'semantic intertexts', namely texts that do not depend on a similarity of word form, but on a similarity of meaning. In this case, we are going to abstract away from the phrase's lexical composition, with a query that will look something like this:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "proximity_glosses = f\"[en?icy] AND [en?frost]\"\n",
    "search = searcher.search(proximity_glosses)\n",
    "\n",
    "# Display the query if any matches\n",
    "if search.count != (0, 0, 0):  # matches, docs, corpora\n",
    "    for hlite in search.highlights:\n",
    "        pprint([hlite.author,\n",
    "               hlite.title,\n",
    "               hlite.reference,\n",
    "               hlite.text])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}