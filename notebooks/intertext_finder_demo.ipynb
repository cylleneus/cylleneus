{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cylleneus + NLP\n",
    "===============\n",
    "\n",
    "Once installed, the Cylleneus engine can be used to perform queries programmatically via the search API. In this way, the engine can be used to build NLP analysis pipelines. One very simple and straightforward use of Cylleneus' search functionality would be to try to find 'intertexts' -- passages of a target text that are lexically similar to some source text -- programmatically. Here's a very basic implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get everything set up\n",
    "import codecs\n",
    "import json\n",
    "from copy import copy\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from textwrap import wrap\n",
    "\n",
    "import multiwordnet\n",
    "from tqdm import tqdm\n",
    "\n",
    "from cylleneus.corpus import Corpus, Work\n",
    "from cylleneus.search import Collection, Searcher\n",
    "from cylleneus.settings import CORPUS_DIR\n",
    "\n",
    "# Because we are going to be processing a text to serve as the\n",
    "# source for constructing queries programatically, we need some\n",
    "# nuts-and-bolts text-handling tools.\n",
    "from cylleneus.engine.analysis.filters import CachedLemmaFilter, CachedSynsetFilter\n",
    "\n",
    "# The source text will come from the Perseus Digital Library (in\n",
    "# JSON format), so can use the bespoke tokenizer for this corpus.\n",
    "from cylleneus.corpus.lat.perseus import Tokenizer\n",
    "\n",
    "# Check MultiWordNet installation\n",
    "for language in [\"common\", \"english\", \"french\", \"hebrew\", \"italian\", \"latin\", \"spanish\"]:\n",
    "    if not multiwordnet.db.exists(language):\n",
    "        multiwordnet.db.compile(language, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ngram extraction\n",
    "\n",
    "To find possible intertexts, we are going to process a source text line by line, using some code adapted from the NLTK to extract a series of ngrams (of specifiable length) to search in the target texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a padded sequence of items before ngram extraction.\n",
    "def pad_sequence(\n",
    "    sequence,\n",
    "    n,\n",
    "    pad_left=False,\n",
    "    pad_right=False,\n",
    "    left_pad_symbol=None,\n",
    "    right_pad_symbol=None,\n",
    "):\n",
    "    sequence = iter(sequence)\n",
    "    if pad_left:\n",
    "        sequence = chain((left_pad_symbol,) * (n - 1), sequence)\n",
    "    if pad_right:\n",
    "        sequence = chain(sequence, (right_pad_symbol,) * (n - 1))\n",
    "    return sequence\n",
    "\n",
    "# Return the ngrams generated from a sequence of items, as an iterator.\n",
    "def ngrams(\n",
    "    sequence,\n",
    "    n,\n",
    "    pad_left=False,\n",
    "    pad_right=False,\n",
    "    left_pad_symbol=None,\n",
    "    right_pad_symbol=None,\n",
    "):\n",
    "    sequence = pad_sequence(\n",
    "        sequence, n, pad_left, pad_right, left_pad_symbol, right_pad_symbol\n",
    "    )\n",
    "\n",
    "    history = []\n",
    "    while n > 1:\n",
    "        try:\n",
    "            next_item = next(sequence)\n",
    "        except StopIteration:\n",
    "            return\n",
    "        history.append(copy(next_item))\n",
    "        n -= 1\n",
    "    for item in sequence:\n",
    "        history.append(copy(item))\n",
    "        yield tuple(history)\n",
    "        del history[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Find similar constructions\n",
    "\n",
    "Once the text has been processed into a series of n-grams, we use Cylleneus' query-building functionality to automatically construct complex phrasal searches. Where possible, each word within a given n-gram is lemmatized in order to abstract away from the specific morphological features of the source text. In the first instance, only the sequential ordering of the n-gram is preserved (as a Sequence query). However, it is possible to loosen this restriction and to look for intertexts where the word order is somehow varied, by specifying a ``slop`` value greater than one (1). Alternatively, Cylleneus can search for 'semantic' intertexts by abstracting away further from the lexicon to query the meanings of words, using ``synsets=True``."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find lexically or semantically similar phrases in the reference corpus\n",
    "# for phrases of length n in a tokenized source text.\n",
    "def find_similar(tokens, collection, n=3, slop=None, synsets=False):\n",
    "    searcher = Searcher(collection=collection)\n",
    "\n",
    "    if not synsets:\n",
    "        before, after = '<', '>'\n",
    "    else:\n",
    "        before, after = '[', ']'\n",
    "\n",
    "    # Generate and process n-grams\n",
    "    for ngram in tqdm(ngrams(tokens, n), desc=\"Searching\"):\n",
    "        queries = []\n",
    "\n",
    "        # Lemmatize, and optionally synsetize, every word in the ngram\n",
    "        for gram in ngram:\n",
    "            subqueries = set()\n",
    "            if not synsets:\n",
    "                lemmatizer = CachedLemmaFilter()\n",
    "                for lemma in lemmatizer([copy(gram),], cached=False, mode='query'):\n",
    "                    subqueries.add(lemma.text.split('=')[0])\n",
    "            else:\n",
    "                synsetizer = CachedSynsetFilter()\n",
    "                for synset in synsetizer(lemmatizer([copy(gram),], mode='query'), mode='query'):\n",
    "                    if synset.text:\n",
    "                        subqueries.add(synset.text)\n",
    "\n",
    "            # Construct subquery\n",
    "            if len(subqueries) == 0:\n",
    "                queries.append(f\"{gram.text}\")\n",
    "            elif len(subqueries) == 1:\n",
    "                queries.append(f\"{before}{list(subqueries)[0]}{after}\")\n",
    "            else:\n",
    "                queries.append(f'''({' OR '.join([f\"{before}{subquery}{after}\" for subquery in subqueries])})''')\n",
    "\n",
    "        # Join all subqueries into a single complex proximity or adjacency query\n",
    "        if slop:\n",
    "            query = f'''\"{' AND '.join(queries)}\"{f'~{slop}' if slop else ''}'''\n",
    "        else:\n",
    "            query = f'''{' THEN '.join(queries)}'''\n",
    "\n",
    "        # Execute the query against the given work\n",
    "        search = searcher.search(query)\n",
    "\n",
    "        if search:\n",
    "            for result in search.to_text():\n",
    "                yield ngram, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source text\n",
    "\n",
    "First, we need to specify the source text.\n",
    "\n",
    "##### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available corpora\n",
    "# print(\"\\n\".join([f'{name} [{meta.language}] {meta.description}' for name, meta in manifest.items() if meta.repo[\"location\"] == \"remote\"]))\n",
    "\n",
    "corpus = Corpus(\"perseus\")\n",
    "if not corpus.searchable:\n",
    "    corpus.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List works in selected corpus\n",
    "for docix, work in sorted(corpus.manifest.items(), key=lambda x: x[0]):\n",
    "    print(f\"[{docix}] {work['author']}, {work['title']}\")\n",
    "\n",
    "# Select the source text by document index number\n",
    "work = corpus.work_by_docix(0)  # Vergil's Aeneid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target texts\n",
    "\n",
    "Next, we create a ``Collection`` of target texts (including from different corpora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a collection of texts in which to search\n",
    "collection = Collection()\n",
    "collection.add(corpus.work_by_docix(1))  # Vergil's Eclogues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For very long texts, this might take a while!\n",
    "results = []\n",
    "for filename in work.filename:\n",
    "    # Load target text\n",
    "    with codecs.open(work.corpus.text_dir / filename, 'r', 'utf8') as fp:\n",
    "        doc = json.load(fp)\n",
    "    meta = work.meta\n",
    "\n",
    "    # Tokenize the target text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokens = tokenizer({ \"text\": doc['text'], \"meta\": meta }, mode='index')\n",
    "\n",
    "    # Use the token list to create n-grams of length n, and search for these in the target text\n",
    "    hlites = find_similar(tokens, collection, n=3)\n",
    "    results.extend([hlite for hlite in hlites])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results...\n",
    "print(f\"{work.author}, {work.title}\")\n",
    "for ngram, (corpus, author, title, urn, reference, text) in results:\n",
    "    print(f\"\\n{'-'.join(set([', '.join([f'{k}: {v}' for k, v in token.meta.items() if k in token.meta['meta'].split('-')]) for token in ngram]))}:\", \", \".join([f\"'{token.text}'\" for token in ngram]))\n",
    "    print(f\"{author}, {title} {reference}\")\n",
    "    subs = [(\"<pre>\", \"\"), (\"</pre>\", \"\"), (\"<match>\", \"\"), (\"</match>\", \"\"), (\"<post>\", \"\"), (\"</post>\", \"\"), (\"<em>\", \"*\"), (\"</em>\", \"*\")]\n",
    "    for pat, sub in subs:\n",
    "        text = text.replace(pat, sub)\n",
    "    print(\"\\n\".join(wrap(text)))\n",
    "\n",
    "# ... or save the results locally\n",
    "# with codecs.open(\"\", \"w\", \"utf8\") as fp:  # Add file name here\n",
    "#     fp.write(f\"{work.author.upper()}, {work.title.upper()}\\n\")\n",
    "#     for ngram, (corpus, author, title, urn, reference, text) in results:\n",
    "#         fp.write(f\"\\n\\n{'-'.join(set([', '.join([f'{k}: {v}' for k, v in token.meta.items() if k in token.meta['meta'].split('-')]) for token in ngram]))}:\", \", \".join([f\"'{token.text}'\" for token in ngram]))\n",
    "#         fp.write(f\"\\n{author}, {title} {reference}\")\n",
    "#         subs = [(\"<pre>\", \"\"), (\"</pre>\", \"\"), (\"<match>\", \"\"), (\"</match>\", \"\"), (\"<post>\", \"\"), (\"</post>\", \"\"), (\"<em>\", \"*\"), (\"</em>\", \"*\")]\n",
    "#         for pat, sub in subs:\n",
    "#             text = text.replace(pat, sub)\n",
    "#         print(\"\\n\".join(wrap(text)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}