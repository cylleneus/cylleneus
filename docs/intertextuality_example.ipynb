{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cylleneus + NLP\n",
    "===============\n",
    "\n",
    "Probably most often, the Cylleneus search engine will be used through one of its more user-friendly interfaces. However, it is also possible to use the engine as an API and to perform queries programmatically. In this way, the engine can in fact be used to build NLP applications. One very simple and straightforward use of Cylleneus' lemma-based query functionality would be to try to find 'intertexts' -- passages of text that are lexically similar to, but not morphologically identical to, some source text. Let's try it out.\n",
    "\n",
    "First, set up the search environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need tell Cylleneus what corpus we want to search, and then instantiate a Searcher object to execute specific queries.\n",
    "from corpus import Corpus\n",
    "from search import Searcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Perseus Digital Library minicorpus that comes pre-installed with the Cylleneus repository; it includes the major works of Vergil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus('perseus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to abstract away morphological details of our source text, we are also going to need to tokenize and lemmatize this text. In this case, for simplicity's sake, we will just be searching for a single phrase, which we can input manually. Since the text isn't coming from a structured corpus, we can use the built-in plaintext tokenizer and lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plaintext tokenizer should be suitable for just about any 'raw' Latin text.\n",
    "from engine.analysis.tokenizers import CachedPlainTextTokenizer\n",
    "\n",
    "# The lemma filter takes a sequence of tokens (word-forms) and uses the Latin WordNet for lemmatization and morphological analysis.\n",
    "from engine.analysis.filters import CachedLemmaFilter\n",
    "\n",
    "tokenizer = CachedPlainTextTokenizer()\n",
    "lemmatizer = CachedLemmaFilter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our source text through our lemmatization pipeline. In this fabricated example, we are going to search for texts similar to the phrase of Lucretius: *gelidamque pruinam* (Lucr. *RN.* 2.431)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For efficiency the tokenizer reuses a single Token object, so each token needs to be copied to be preserved\n",
    "from copy import copy\n",
    "\n",
    "text = 'gelidamque pruinam'\n",
    "tokens = [copy(token) for token in tokenizer(text, mode='index')]\n",
    "\n",
    "lemmas = []\n",
    "for token in tokens:\n",
    "    lemmas.append(list(set([lemma.text for lemma in lemmatizer([token,], mode='query')])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we need to construct a well-formed lemma-based query for Cylleneus to execute. In the most basic kind of query, we would simply combine the lemmatized tokens together as a sequence.\n",
    "\n",
    "NB. The lemmatizer tries to be inclusive as possible, so a form like *fatis* will generate multiple lemmas for possible matching: *fatum* as well as *fatis* and *fatus*. This is why, if we were to inspect the `lemmas` object, we would find that each word of the original text resolves to a list of lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['gelida=n-s---fn1-', 'gelidus=aps---mn1-'],\n",
      " ['que=rp--------'],\n",
      " ['pruina=n-s---fn1-']]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct sequential lemma-based query\n",
    "subqueries = []\n",
    "for i, lemma in enumerate(lemmas):\n",
    "    if len(lemma) == 0:  # no lemma found, use the original form\n",
    "        subqueries.append(tokens[i].text)\n",
    "    elif len(lemma) == 1:\n",
    "        subqueries.append(f\"<{lemma[0]}>\")\n",
    "    else:\n",
    "        subqueries.append(f'''({' OR '.join([f\"<{alt}>\" for alt in lemma])})''')\n",
    "\n",
    "# Join all subqueries into a single adjacency query\n",
    "adjacent_lemmas = f'''\"{' '.join(subqueries)}\"'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be more inclusive, we could do away entire with the sequential requirement and try instead using a proximity query. In this case, any text will match provided only that it contains the matching query terms, irrespective of their ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "proximal_lemmas = f'''{' '.join(subqueries)}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n',\n",
      " 'VIRGIL, GEORGICS\\n',\n",
      " 'poem: 2, line: 367-poem: 2, line: 368\\n',\n",
      " '    intereunt pecudes, stant circumfusa pruinis\\n',\n",
      " '\\n',\n",
      " 'poem: 1, line: 487\\n',\n",
      " '    Taygeta! O, qui me gelidis convallibus Haemi\\n',\n",
      " '\\n',\n",
      " 'poem: 3, line: 508\\n',\n",
      " '    flesse sibi et gelidis haec evolvisse sub antris\\n',\n",
      " '\\n',\n",
      " 'poem: 0, line: 286\\n',\n",
      " '    Multa adeo gelida melius se nocte dedere,\\n',\n",
      " '\\n',\n",
      " 'poem: 1, line: 262\\n',\n",
      " '    arva solo: id venti curant gelidaeque pruinae\\n',\n",
      " '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Execute the query against the given corpus. `minscore` guarantees that only results\n",
    "# matching all three terms will be returned\n",
    "searcher = Searcher(corpus)\n",
    "search = searcher.search(proximal_lemmas)  \n",
    "\n",
    "# Display the query if any matches\n",
    "if search.count != (0, 0):  # matches, docs\n",
    "    pprint(list(search.highlights))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}