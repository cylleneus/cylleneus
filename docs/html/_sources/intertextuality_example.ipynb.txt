{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cylleneus + NLP\n",
    "===============\n",
    "\n",
    "Probably most often, the Cylleneus search engine will be used through one of its more user-friendly interfaces. However, it is also possible to use the engine as an API and to perform queries programmatically. In this way, the engine can in fact be used to build NLP applications. One very simple and straightforward use of Cylleneus' lemma-based query functionality would be to try to find 'intertexts' -- passages of text that are lexically similar to, but not morphologically identical to, some source text. Let's try it out.\n",
    "\n",
    "First, set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Utility imports\n",
    "import json\n",
    "from lang.latin.stop_words import STOP_WORDS\n",
    "\n",
    "# We need tell Cylleneus what corpus we want to search, and then instantiate a Searcher object to execute specific queries.\n",
    "from corpus import Corpus\n",
    "from search import Searcher, Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Perseus Digital Library minicorpus that comes pre-installed with the Cylleneus repository; it includes the major works of Vergil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "corpus = Corpus('perseus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to abstract away morphological details of our source text, we are also going to need to tokenize and lemmatize this text. In this case, for simplicity's sake, we will just be searching for a single phrase, which we can input manually. Since the text isn't coming from a structured corpus, we can use the built-in plaintext tokenizer and lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# The plaintext tokenizer should be suitable for just about any 'raw' Latin text.\n",
    "from engine.analysis.tokenizers import CachedPlainTextTokenizer\n",
    "\n",
    "# The lemma filter takes a sequence of tokens (word-forms) and uses the Latin WordNet for lemmatization and morphological analysis.\n",
    "from engine.analysis.filters import CachedLemmaFilter\n",
    "\n",
    "tokenizer = CachedPlainTextTokenizer()\n",
    "lemmatizer = CachedLemmaFilter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our source text through our lemmatization pipeline. In this fabricated example, we are going to search for texts similar to the phrase of Lucretius: *gelidamque pruinam* (Lucr. *RN.* 2.431)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# For efficiency the tokenizer reuses a single Token object, so each token needs to be copied to be preserved\n",
    "from copy import copy\n",
    "\n",
    "text = 'gelidamque pruinam'\n",
    "tokens = [copy(token) for token in tokenizer(text, mode='index') if token.text not in STOP_WORDS['CONJUNCTIONS']]\n",
    "\n",
    "lemmas = []\n",
    "for token in tokens:\n",
    "    lemmas.append(list(set([lemma.text.split(':')[0] for lemma in lemmatizer([token,], mode='query')])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we need to construct a well-formed lemma-based query for Cylleneus to execute. In the most basic kind of query, we would simply combine the lemmatized tokens together as a sequence.\n",
    "\n",
    "NB. The lemmatizer tries to be inclusive as possible, so a form like *fatis* will generate multiple lemmas for possible matching: *fatum* as well as *fatis* and *fatus*. This is why, if we were to inspect the `lemmas` object, we would find that each word of the original text resolves to a list of lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[['gelidus', 'gelida'], ['pruina']]",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Construct sequential lemma-based query\n",
    "subqueries = []\n",
    "for i, lemma in enumerate(lemmas):\n",
    "    if len(lemma) == 0:  # no lemma found, use the original form\n",
    "        subqueries.append(tokens[i].text)\n",
    "    elif len(lemma) == 1:\n",
    "        subqueries.append(f\"<{lemma[0]}>\")\n",
    "    else:\n",
    "        subqueries.append(f'''({' OR '.join([f\"<{alt}>\" for alt in lemma])})''')\n",
    "\n",
    "# Join all subqueries into a single adjacency query\n",
    "adjacent_lemmas = f'''\"{' '.join(subqueries)}\"'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be more inclusive, we could do away entire with the sequential requirement and try instead using a proximity query. In this case, any text will match provided only that it contains the matching query terms, irrespective of their ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "'(<gelidus> OR <gelida>) <pruina>'",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "proximal_lemmas = f'''{' '.join(subqueries)}'''\n",
    "pprint(proximal_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[",
      "'Virgil'",
      ",\n ",
      "'Georgics'",
      ",\n ",
      "'poem: 2, line: 263'",
      ",\n ",
      "'<pre>ante supinatas aquiloni ostendere glaebas,</pre>\\n'",
      "\n ",
      "'\\n'",
      "\n ",
      "'<pre>quam laetum infodias vitis genus. Optima putri</pre>\\n'",
      "\n ",
      "'\\n'",
      "\n ",
      "'<match>arva solo: id venti curant <em>gelidaeque</em> '",
      "\n ",
      "'<em>pruinae</em></match>\\n'",
      "\n ",
      "'\\n'",
      "\n ",
      "'<post>et labefacta movens robustus iugera fossor.</post>\\n'",
      "\n ",
      "'\\n'",
      "\n ",
      "'<post>Ac si quos haud ulla viros vigilantia fugit,</post>'",
      "]",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Execute the query against the given collection of texts.\n",
    "searcher = Searcher(Collection(corpus.works))\n",
    "search = searcher.search(proximal_lemmas)  \n",
    "\n",
    "# Display the query if any matches\n",
    "if search.count != (0, 0, 0):  # matches, docs, corpora\n",
    "    for result in json.loads(search.to_json())['results']:\n",
    "        pprint([result['author'],\n",
    "               result['title'],\n",
    "               result['reference'],\n",
    "               result['text']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
